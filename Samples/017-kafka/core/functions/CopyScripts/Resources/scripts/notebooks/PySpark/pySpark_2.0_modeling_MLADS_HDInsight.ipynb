{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exploration and Modeling of Sampled 2013 NYC Taxi Trip and Fare Dataset in Spark 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook shows the basic data science steps for Spark. It demos features of Spark's MLlib toolkit using the NYC taxi trip and fare data-set from 2013. We take a 0.1% sample of this data-set (about 170K rows, 35 Mb) to to show MLlib's modeling features for binary classification.\n",
    "\n",
    "Many thanks to Debraj GuhaThakurta who developed significant portions of this content. A longer version of this notebook is available on the Linux DSVM. Log in to JupyterHub, then navigate to SparkML -> pySpark -> pySpark modeling.ipynb.\n",
    "\n",
    "A similar tutorial is available for HDInsight on the [Cloud Intelligence and Machine Learning blog](https://blogs.technet.microsoft.com/machinelearning/2017/03/22/end-to-end-data-science-walkthrough-with-spark-2-0-on-azure-hdinsight-hadoop-clusters/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows data ingestion, exploration and plotting, data preparation (featurizing/transformation), modeling, prediction, model persistance and model evaluation on an independent validation data-set. Machine learning tasks are performed using Spark's MLlib functions. For plotting purposes, the Spark dataframes are converted to pandas dataframes so matplotlib functions can be used. This is because there are no good Spark libraries for creating plots from Spark dataframes.\n",
    "\n",
    "We address a single regression problem: predicting the amount of tip paid for taxi trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "from IPython.display import Image\n",
    "Image(\"https://dsvmassets.blob.core.windows.net/images/spark_process.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1. Location of training data. This could also be in ADLS\n",
    "taxi_train_file_loc = \"wasb://nyctaxi@dsvmdemo.blob.core.windows.net/JoinedTaxiTripFare.Point1Pct.Train.csv\"\n",
    "taxi_valid_file_loc = \"wasb://nyctaxi@dsvmdemo.blob.core.windows.net/JoinedTaxiTripFare.Point1Pct.Valid.csv\"\n",
    "\n",
    "# 2. Set model storage directory path. This is where models will be saved.\n",
    "modelDir = \"wasb://nyctaxi@dsvmdemo.blob.core.windows.net/Outputs/\" # The last backslash is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, RegressionMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, RFormula\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import atexit\n",
    "from sklearn.metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data ingestion\n",
    "\n",
    "Read in joined 0.1% taxi trip and fare file (as csv), format and clean data, and create data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Read in CSV data as a Spark dataframe\n",
    "taxi_train_df = spark.read.csv(path=taxi_train_file_loc, header=True, inferSchema=True)\n",
    "\n",
    "## Drop some unnecessary columns\n",
    "taxi_df_train_cleaned = taxi_train_df.drop('medallion').drop('hack_license')\n",
    "    \n",
    "## filter out undesirable values and outliers\n",
    "taxi_df_train_cleaned = taxi_df_train_cleaned.filter(\"passenger_count > 0 and passenger_count < 8\")\n",
    "\n",
    "## Register the dataframe as a temp table in the SQL context\n",
    "taxi_df_train_cleaned.createOrReplaceTempView(\"taxi_train\")\n",
    "\n",
    "taxi_df_train_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data exploration & visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Plot histogram of tip amount, relationship between tip amount vs. other features\n",
    "\n",
    "\n",
    "For plotting, the dataframe will first have to be converted to a pandas dataframe so matplotlib can use it for generating plots. Here, if the Spark dataframe is large, it can be down-sampled (using the \"sample\" function). In the example below, 50% of data was sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql -q -o sqlResultsPD\n",
    "SELECT fare_amount, passenger_count, tip_amount, tipped FROM taxi_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## tip by payment type and passenger count\n",
    "ax1 = sqlResultsPD[['tip_amount']].plot(kind='hist', bins=25, facecolor='lightblue')\n",
    "ax1.set_title('Tip amount distribution')\n",
    "ax1.set_xlabel('Tip Amount ($)'); ax1.set_ylabel('Counts')\n",
    "plt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n",
    "\n",
    "## tip amount by fare amount. Points are scaled by passenger count\n",
    "ax = sqlResultsPD.plot(kind='scatter', x= 'fare_amount', y = 'tip_amount', c='blue', alpha = 0.10, s=2.5*(sqlResultsPD.passenger_count))\n",
    "ax.set_title('Tip amount by Fare amount')\n",
    "ax.set_xlabel('Fare Amount ($)'); ax.set_ylabel('Tip Amount ($)')\n",
    "plt.axis([-2, 80, -2, 20])\n",
    "plt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We leave it up to you to figure out what the vertical line of dots is near the $50 fare amount. Spark SQL makes this straightforward.\n",
    "\n",
    "## Feature engineering, transformation and data prep for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create a new feature by binning hours into traffic time buckets using Spark SQL\n",
    "\n",
    "Spark SQL can be a very convenient way to perform pre-modeling steps, including data transformation, clean-up etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## create four buckets for pickup times\n",
    "sqlStatement = \"\"\" SELECT *, CASE\n",
    "     WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN \"Night\" \n",
    "     WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN \"AMRush\" \n",
    "     WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN \"Afternoon\"\n",
    "     WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN \"PMRush\"\n",
    "    END as TrafficTimeBins\n",
    "    FROM taxi_train \n",
    "\"\"\"\n",
    "taxi_df_train_with_newFeatures = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This next cell is necessary to render tables correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Indexing and one-hot encoding of categorical features\n",
    "\n",
    "Here we only transform a few variables to an example of how to transform strings to one-hot encoding. Other variables, such as weekday, which are represented by numerical values, can also be indexed as categorical variables.\n",
    "\n",
    "For indexing, we used StringIndexer, and for one-hot encoding, we used OneHotEncoder functions from MLlib. StringIndexer converts text values to index values. For example, assume our data is\n",
    "\n",
    "| id | string-value  \n",
    "|----|---------------\n",
    "|  0 | a             \n",
    "|  1 | b             \n",
    "|  2 | b             \n",
    "|  3 | b             \n",
    "|  4 | a             \n",
    "\n",
    "StringIndexer converts this to\n",
    "\n",
    "| id | string-value  | index-value |\n",
    "|----|---------------|-------------|\n",
    "|  0 | a             | 1           |\n",
    "|  1 | b             | 0           |\n",
    "|  2 | b             | 1           |\n",
    "|  3 | b             | 1           |\n",
    "|  4 | a             | 0           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## define the transformations that need to be applied to some of the features\n",
    "sI1 = StringIndexer(inputCol=\"vendor_id\", outputCol=\"vendorIndex\"); en1 = OneHotEncoder(dropLast=False, inputCol=\"vendorIndex\", outputCol=\"vendorVec\");\n",
    "sI2 = StringIndexer(inputCol=\"rate_code\", outputCol=\"rateIndex\"); en2 = OneHotEncoder(dropLast=False, inputCol=\"rateIndex\", outputCol=\"rateVec\");\n",
    "sI3 = StringIndexer(inputCol=\"payment_type\", outputCol=\"paymentIndex\"); en3 = OneHotEncoder(dropLast=False, inputCol=\"paymentIndex\", outputCol=\"paymentVec\");\n",
    "sI4 = StringIndexer(inputCol=\"TrafficTimeBins\", outputCol=\"TrafficTimeBinsIndex\"); en4 = OneHotEncoder(dropLast=False, inputCol=\"TrafficTimeBinsIndex\", outputCol=\"TrafficTimeBinsVec\");\n",
    "\n",
    "## apply the transformations\n",
    "encodedFinal = Pipeline(stages=[sI1, en1, sI2, en2, sI3, en3, sI4, en4]).fit(taxi_df_train_with_newFeatures).transform(taxi_df_train_with_newFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create a random sampling of the data, as needed (50% is used here). This can save time while training models. Then, split into train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainingFraction = 0.5\n",
    "seed = 1234;\n",
    "encodedFinalSampled = encodedFinal.sample(False, 0.5, seed=seed)\n",
    "\n",
    "## split sampled dataframe into train and test\n",
    "trainData, testData = encodedFinalSampled.randomSplit([trainingFraction, 1.0 - trainingFraction], seed=seed);\n",
    "\n",
    "## cache the dataframes in memory\n",
    "trainData.cache(); trainData.count();\n",
    "testData.cache(); testData.count();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Regression model training: Predicting amount of tip paid for taxi trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "For modeling, the features and class labels are specified using the convenient RFormula function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Define regression formula\n",
    "regFormula = RFormula(formula=\"tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + weekday + passenger_count + trip_time_in_secs + trip_distance + fare_amount\")\n",
    "\n",
    "## Define indexer for categorical variables\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n",
    "\n",
    "## Random forest estimator\n",
    "randForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label', numTrees=20, \n",
    "                                   featureSubsetStrategy=\"auto\",impurity='variance', maxDepth=6, maxBins=100)\n",
    "\n",
    "## Fit model, with formula and other transformations\n",
    "model = Pipeline(stages=[regFormula, featureIndexer, randForest]).fit(trainData)\n",
    "\n",
    "## PREDICT ON TEST DATA AND EVALUATE\n",
    "predictions = model.transform(testData)\n",
    "predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
    "testMetrics = RegressionMetrics(predictionAndLabels)\n",
    "print(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\n",
    "print(\"R-sqr = %s\" % testMetrics.r2)\n",
    "\n",
    "## PLOC ACTUALS VS. PREDICTIONS\n",
    "predictionsPD = predictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql -q -o predictionsPD\n",
    "SELECT * FROM tmp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import numpy as np\n",
    "\n",
    "ax = predictionsPD.plot(kind='scatter', figsize = (5,5), x='label', y='prediction', color='blue', alpha = 0.15, label='Actual vs. predicted');\n",
    "fit = np.polyfit(predictionsPD['label'], predictionsPD['prediction'], deg=1)\n",
    "ax.set_title('Actual vs. Predicted Tip Amounts ($)')\n",
    "ax.set_xlabel(\"Actual\"); ax.set_ylabel(\"Predicted\");\n",
    "ax.plot(predictionsPD['label'], fit[0] * predictionsPD['label'] + fit[1], color='magenta')\n",
    "plt.axis([-1, 15, -1, 15])\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save the model, then load it and evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## skip saving the model, as it requires write access to WASB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyper-parameter tuning: Train a random forest model using hyper-parameter tuning and cross-validation\n",
    "\n",
    "Notice that as expected, the parameter tuning and cross-validation improves the model performance (R-sqr) significantly on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## DEFINE RANDOM FOREST MODELS\n",
    "randForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label', \n",
    "                                   featureSubsetStrategy=\"auto\",impurity='variance', maxBins=100)\n",
    "\n",
    "## DEFINE MODELING PIPELINE, INCLUDING FORMULA, FEATURE TRANSFORMATIONS, AND ESTIMATOR\n",
    "pipeline = Pipeline(stages=[regFormula, featureIndexer, randForest])\n",
    "\n",
    "## DEFINE PARAMETER GRID FOR RANDOM FOREST\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(randForest.numTrees, [10, 25, 50]) \\\n",
    "    .addGrid(randForest.maxDepth, [3, 5, 7]) \\\n",
    "    .build()\n",
    "\n",
    "## DEFINE CROSS VALIDATION\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "## TRAIN MODEL USING CV\n",
    "cvModel = crossval.fit(trainData)\n",
    "\n",
    "## PREDICT AND EVALUATE TEST DATA SET\n",
    "predictions = cvModel.transform(testData)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R-squared on test data = %g\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load independent validation data-set and evaluate a model\n",
    "\n",
    "The validation data-set needs to be transformed in the same way as the training data in order to score it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## READ IN DATA FRAME FROM CSV\n",
    "taxi_valid_df = spark.read.csv(path=taxi_valid_file_loc, header=True, inferSchema=True)\n",
    "\n",
    "## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\n",
    "taxi_df_valid_cleaned = taxi_valid_df.drop('medallion').drop('hack_license').drop('store_and_fwd_flag').drop('pickup_datetime')\\\n",
    "    .drop('dropoff_datetime').drop('pickup_longitude').drop('pickup_latitude').drop('dropoff_latitude')\\\n",
    "    .drop('dropoff_longitude').drop('tip_class').drop('total_amount').drop('tolls_amount').drop('mta_tax')\\\n",
    "    .drop('direct_distance').drop('surcharge')\\\n",
    "    .filter(\"passenger_count > 0 and passenger_count < 8 AND payment_type in ('CSH', 'CRD') \\\n",
    "        AND tip_amount >= 0 AND tip_amount < 30 AND fare_amount >= 1 AND fare_amount < 200 \\\n",
    "        AND trip_distance > 0 AND trip_distance < 100 AND trip_time_in_secs > 30 AND trip_time_in_secs < 7200\")\n",
    "\n",
    "## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\n",
    "taxi_df_valid_cleaned.createOrReplaceTempView(\"taxi_valid\")\n",
    "\n",
    "### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\n",
    "sqlStatement = \"\"\" SELECT *, CASE\n",
    "     WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN \"Night\" \n",
    "     WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN \"AMRush\" \n",
    "     WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN \"Afternoon\"\n",
    "     WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN \"PMRush\"\n",
    "    END as TrafficTimeBins\n",
    "    FROM taxi_valid\n",
    "\"\"\"\n",
    "taxi_df_valid_with_newFeatures = spark.sql(sqlStatement)\n",
    "\n",
    "## APPLY THE SAME TRANSFORATION ON THIS DATA AS ORIGINAL TRAINING DATA\n",
    "encodedFinalValid = Pipeline(stages=[sI1, en1, sI2, en2, sI3, en3, sI4, en4]).fit(taxi_df_train_with_newFeatures).transform(taxi_df_valid_with_newFeatures)\n",
    "\n",
    "# predict using the cross-validation model\n",
    "predictions = cvModel.bestModel.transform(encodedFinalValid)\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R-squared on test data = %g\" % r2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2 Spark - HDInsight",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
